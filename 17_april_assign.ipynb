{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c509cf9-0a83-40f5-899f-34e94ad6b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "    Ans: Gradient Boosting Regression is a machine learning technique that combines multiple weak regression models in an additive manner. It trains models in a stage-wise \n",
    "         fashion, with each model fitting to the negative gradient of the loss function. The final model is an ensemble of weak models, producing accurate regression predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1688f6a-1b05-4427-bb8e-db017b410ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. \n",
    "       Evaluate the model's performance using metrics such as mean squared error and R-squared.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56901fc9-0156-4a7c-b9e7-d894c4cafa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 31.735482349161565\n",
      "R-squared: 0.9772379183627112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.mean(y)  # Initialize with the mean\n",
    "        residuals = y - y_pred\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # Update weights\n",
    "            self.weights.append(self.learning_rate)\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "            residuals = y - y_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            y_pred += weight * model.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'n_estimators': self.n_estimators, 'learning_rate': self.learning_rate, 'max_depth': self.max_depth}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "# Example usage\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate toy dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred = gb_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7048df-fbd9-43cb-8d09-4cd85ca902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random \n",
    "       search to find the best hyperparameters.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22af8252-59aa-4a39-9bbf-98100383b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Model Performance:\n",
      "Mean Squared Error: 31.735482349161565\n",
      "R-squared: 0.9772379183627112\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,150],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3,4]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "random_search = GridSearchCV(gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Performance:\")\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370b317-af7f-4bd5-82c6-0b97958cbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "    Ans: A weak learner in gradient boosting is a simple and relatively low-performing model that can be trained to make predictions slightly better than random guessing. \n",
    "         It typically has low complexity, such as a decision tree with shallow depth, and is combined with other weak learners to create a strong ensemble model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e7ce0-e181-4b51-a746-643cabb5cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "    Ans: The intuition behind the Gradient Boosting algorithm is to iteratively build an ensemble of weak models, where each subsequent model focuses on reducing the errors \n",
    "         made by the previous models. It does this by fitting each new model to the negative gradient (residuals) of the loss function of the ensemble. By continuously adjusting\n",
    "         the weights of the weak models, Gradient Boosting aims to create a strong learner that gradually minimizes the overall loss and improves predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24cb23-ba94-4de3-878b-9243b3294b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "    Ans: The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. It starts with an initial prediction based on the average value of the \n",
    "         target variable. In each iteration, a new weak learner is trained to predict the negative gradient (residuals) of the loss function with respect to the current \n",
    "         ensemble's predictions. The predictions of all weak learners are combined with a weight assigned to each learner to create the final ensemble prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc647d95-ab09-47a8-9602-b3a6d55e33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "    Ans: 1) Initialization with a simple model \n",
    "         2) Calculation of residuals (negative gradients)\n",
    "         3) Fitting a weak learner to the residuals \n",
    "         4) Updating the ensemble by adding the weak learner's predictions with a weight\n",
    "         5) Iteratively repeating these process multiple times.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
